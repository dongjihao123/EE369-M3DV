{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import (Conv3D, BatchNormalization, AveragePooling3D, concatenate, Lambda,\n",
    "                          Activation, SpatialDropout3D, Input, GlobalAvgPool3D, Dense)\n",
    "from keras.regularizers import l2 as l2_penalty\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard, ModelCheckpoint,Callback\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "#from mylib.models.metrics import invasion_acc, invasion_precision, invasion_recall, invasion_fmeasure\n",
    "\n",
    "PARAMS = {\n",
    "    'activation': lambda: Activation('relu'),  # the activation functions\n",
    "    'bn_scale': True,  # whether to use the scale function in BN\n",
    "    'weight_decay': 0.,  # l2 weight decay\n",
    "    'kernel_initializer': 'he_uniform',  # initialization\n",
    "    'first_scale':None,   # lambda x: x / 128. - 1.,  # the first pre-processing function\n",
    "    'dhw': [32, 32, 32],  # the input shape\n",
    "    'k': 16,  # the `growth rate` in DenseNet\n",
    "    'bottleneck': 2,  # the `bottleneck` in DenseNet\n",
    "    'compression': 2,  # the `compression` in DenseNet\n",
    "    'first_layer': 32,  # the channel of the first layer\n",
    "    'down_structure': [4, 4],  # the down-sample structure\n",
    "    'output_size': 2  # the output number of the classification head\n",
    "}\n",
    "\n",
    "\n",
    "def _conv_block(x, filters):\n",
    "    bn_scale = PARAMS['bn_scale']\n",
    "    activation = PARAMS['activation']\n",
    "    kernel_initializer = PARAMS['kernel_initializer']\n",
    "    weight_decay = PARAMS['weight_decay']\n",
    "    bottleneck = PARAMS['bottleneck']\n",
    "\n",
    "    x = BatchNormalization(scale=bn_scale, axis=-1)(x)\n",
    "    x = activation()(x)\n",
    "    x = Conv3D(filters * bottleneck, kernel_size=(1, 1, 1), padding='same', use_bias=False,\n",
    "               kernel_initializer=kernel_initializer,\n",
    "               kernel_regularizer=l2_penalty(weight_decay))(x)\n",
    "    #x = SpatialDropout3D(0.1)(x)\n",
    "    x = BatchNormalization(scale=bn_scale, axis=-1)(x)\n",
    "    x = activation()(x)\n",
    "    x = Conv3D(filters, kernel_size=(3, 3, 3), padding='same', use_bias=True,\n",
    "               kernel_initializer=kernel_initializer,\n",
    "               kernel_regularizer=l2_penalty(weight_decay))(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _dense_block(x, n):\n",
    "    k = PARAMS['k']\n",
    "\n",
    "    for _ in range(n):\n",
    "        conv = _conv_block(x, k)\n",
    "        x = concatenate([conv, x], axis=-1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _transmit_block(x, is_last):\n",
    "    bn_scale = PARAMS['bn_scale']\n",
    "    activation = PARAMS['activation']\n",
    "    kernel_initializer = PARAMS['kernel_initializer']\n",
    "    weight_decay = PARAMS['weight_decay']\n",
    "    compression = PARAMS['compression']\n",
    "\n",
    "    x = BatchNormalization(scale=bn_scale, axis=-1)(x)\n",
    "    x = activation()(x)\n",
    "    if is_last:\n",
    "        x = GlobalAvgPool3D()(x)\n",
    "    else:\n",
    "        *_, f = x.get_shape().as_list()\n",
    "        x = Conv3D(f // compression, kernel_size=(1, 1, 1), padding='same', use_bias=True,\n",
    "                   kernel_initializer=kernel_initializer,\n",
    "                   kernel_regularizer=l2_penalty(weight_decay))(x)\n",
    "        x = AveragePooling3D((2, 2, 2), padding='valid',)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_model(weights=None, **kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        assert k in PARAMS\n",
    "        PARAMS[k] = v\n",
    "    print(\"Model hyper-parameters:\", PARAMS)\n",
    "\n",
    "    dhw = PARAMS['dhw']\n",
    "    first_scale = PARAMS['first_scale']\n",
    "    first_layer = PARAMS['first_layer']\n",
    "    kernel_initializer = PARAMS['kernel_initializer']\n",
    "    weight_decay = PARAMS['weight_decay']\n",
    "    down_structure = PARAMS['down_structure']\n",
    "    output_size = PARAMS['output_size']\n",
    "\n",
    "    shape = dhw + [1]\n",
    "\n",
    "    inputs = Input(shape=shape)\n",
    "\n",
    "    if first_scale is not None:\n",
    "        scaled = Lambda(first_scale)(inputs)\n",
    "    else:\n",
    "        scaled = inputs\n",
    "    conv = Conv3D(first_layer, kernel_size=(3, 3, 3), padding='same', use_bias=True,\n",
    "                  kernel_initializer=kernel_initializer,\n",
    "                  kernel_regularizer=l2_penalty(weight_decay))(scaled)\n",
    "\n",
    "    downsample_times = len(down_structure)\n",
    "    for l, n in enumerate(down_structure):\n",
    "        db = _dense_block(conv, n)\n",
    "        conv = _transmit_block(db, l == downsample_times - 1)\n",
    "\n",
    "    if output_size == 1:\n",
    "        last_activation = 'sigmoid'\n",
    "    else:\n",
    "        last_activation = 'softmax'\n",
    "\n",
    "    outputs = Dense(output_size, activation=last_activation,\n",
    "                    kernel_regularizer=l2_penalty(weight_decay),\n",
    "                    kernel_initializer=kernel_initializer)(conv)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.summary()\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights, by_name=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from itertools import repeat\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def rotation(array, angle):\n",
    "    '''using Euler angles method.\n",
    "    @author: renchao\n",
    "    @params:\n",
    "        angle: 0: no rotation, 1: rotate 90 deg, 2: rotate 180 deg, 3: rotate 270 deg\n",
    "    '''\n",
    "    #\n",
    "    X = np.rot90(array, angle[0], axes=(0, 1))  # rotate in X-axis\n",
    "    Y = np.rot90(X, angle[1], axes=(0, 2))  # rotate in Y'-axis\n",
    "    Z = np.rot90(Y, angle[2], axes=(1, 2))  # rotate in Z\"-axis\n",
    "    return Z\n",
    "\n",
    "\n",
    "def reflection(array, axis):\n",
    "    '''\n",
    "    @author: renchao\n",
    "    @params:\n",
    "        axis: -1: no flip, 0: Z-axis, 1: Y-axis, 2: X-axis\n",
    "    '''\n",
    "    if axis != -1:\n",
    "        ref = np.flip(array, axis)\n",
    "    else:\n",
    "        ref = np.copy(array)\n",
    "    return ref\n",
    "\n",
    "\n",
    "def crop(array, zyx, dhw):\n",
    "    z, y, x = zyx\n",
    "    d, h, w = dhw\n",
    "    cropped = array[z - d // 2:z + d // 2,\n",
    "              y - h // 2:y + h // 2,\n",
    "              x - w // 2:x + w // 2]\n",
    "    return cropped\n",
    "\n",
    "\n",
    "def random_center(shape, move):\n",
    "    offset = np.random.randint(-move, move + 1, size=3)\n",
    "    zyx = np.array(shape) // 2 + offset\n",
    "    return zyx\n",
    "\n",
    "\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "\n",
    "    return parse\n",
    "\n",
    "\n",
    "_single = _ntuple(1)\n",
    "_pair = _ntuple(2)\n",
    "_triple = _ntuple(3)\n",
    "_quadruple = _ntuple(4)\n",
    "\n",
    "class Transform:\n",
    "    '''The online data augmentation, including:\n",
    "    1) random move the center by `move`\n",
    "    2) rotation 90 degrees increments\n",
    "    3) reflection in any axis\n",
    "    '''\n",
    "\n",
    "    def __init__(self, size, move):\n",
    "        self.size = _triple(size)\n",
    "        self.move = move\n",
    "\n",
    "    def __call__(self, arr, aux=None):\n",
    "        shape = arr.shape\n",
    "        if self.move is not None:\n",
    "            center = random_center(shape, self.move)\n",
    "            arr_ret = crop(arr, center, self.size)\n",
    "            angle = np.random.randint(4, size=3)\n",
    "            arr_ret = rotation(arr_ret, angle=angle)\n",
    "            axis = np.random.randint(4) - 1\n",
    "            arr_ret = reflection(arr_ret, axis=axis)\n",
    "            arr_ret = np.expand_dims(arr_ret, axis=0)\n",
    "            if aux is not None:\n",
    "                aux_ret = crop(aux, center, self.size)\n",
    "                aux_ret = rotation(aux_ret, angle=angle)\n",
    "                aux_ret = reflection(aux_ret, axis=axis)\n",
    "                aux_ret = np.expand_dims(aux_ret, axis=0)\n",
    "                return arr_ret, aux_ret\n",
    "            return arr_ret\n",
    "        else:\n",
    "            center = np.array(shape) // 2\n",
    "            arr_ret = crop(arr, center, self.size)\n",
    "            arr_ret = np.expand_dims(arr_ret, axis=0)\n",
    "            if aux is not None:\n",
    "                aux_ret = crop(aux, center, self.size)\n",
    "                aux_ret = np.expand_dims(aux_ret, axis=0)\n",
    "                return arr_ret, aux_ret\n",
    "            return arr_ret\n",
    "        \n",
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor1, monitor2,value,verbose):\n",
    "        super(Callback,self).__init__()\n",
    "        self.monitor1 = monitor1\n",
    "        self.monitor2 = monitor2\n",
    "        \n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        current=logs.get(self.monitor1)**0.5+logs.get(self.monitor2)**2\n",
    "        \n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping warning\",RuntimeWarning)\n",
    "        if current < self.value:\n",
    "            if self.verbose>0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading test_data: 100%|██████████| 584/584 [00:09<00:00, 59.27it/s] \n",
      "croping: 100%|██████████| 116/116 [00:00<00:00, 794.82it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras,os,csv\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import zoom\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard, ModelCheckpoint,Callback\n",
    "\n",
    "# 读取训练数据\n",
    "voxel_train = []   #用于存储训练数据的voxel\n",
    "seg_train = []     #用于存储训练数据的seg\n",
    "\n",
    "for i in tqdm(range(584), desc='reading train_data'):           #写入训练数据进度\n",
    "    try:\n",
    "        tmp = np.load('train_val/candidate{}.npz'.format(i))    #依次读取训练数据中的candidate{i}文件\n",
    "    except FileNotFoundError:                                   #无该文件时直接进入下一次循环\n",
    "        continue\n",
    "    try:\n",
    "        voxel_train = np.append(voxel_train, np.expand_dims(tmp['voxel'], axis=0), axis=0)  #向voxel_train中添加读取的voxel向量\n",
    "        seg_train = np.append(seg_train, np.expand_dims(tmp['seg'], axis=0), axis=0)        #向seg_train中添加读取的seg向量\n",
    "    except ValueError:\n",
    "        voxel_train = np.expand_dims(tmp['voxel'], axis=0)      #写入初次读取的voxel\n",
    "        seg_train = np.expand_dims(tmp['seg'], axis=0)          #写入初次读取的seg\n",
    "\n",
    "#读取训练集的标签\n",
    "train_label = pd.read_csv('train_val.csv').values[:, 1].astype(int)\n",
    "#print(train_label.shape)\n",
    "train_label = np.concatenate((train_label,train_label),axis=0)\n",
    "#print(train_label.shape)\n",
    "\n",
    "seg_train = seg_train.astype(np.int)         #将seg的布尔array转换为1/0整数\n",
    "X_train = voxel_train*seg_train              #抠取结节\n",
    "\n",
    "X_train=X_train.astype(np.float32)\n",
    "X_train/=128.-1.\n",
    "\n",
    "training_batch_size = X_train.shape[0]       #训练数据集的数量\n",
    "#print(X_train.shape)\n",
    "\n",
    "X_train_new=crop(X_train[0],(50,50,50),(32,32,32))\n",
    "\n",
    "#print(X_train_new.shape) \n",
    "X_train_new=np.expand_dims(X_train_new,axis=0)\n",
    "#print(X_train_new.shape) \n",
    "\n",
    "for i in tqdm(range(training_batch_size-1),desc='croping'):    #将数据大小截取为(32,32,32)\n",
    "    X_train_new=np.append(X_train_new,np.expand_dims(crop(X_train[i+1],(50,50,50),(32,32,32)),axis=0),axis=0)\n",
    "#print(X_train_new.shape)   \n",
    "\n",
    "for i in tqdm(range(training_batch_size),desc='transforming'): #对数据做transform处理\n",
    "    X_train_new=np.append(X_train_new,Transform(32,5)(X_train[i]),axis=0)\n",
    "#print(X_train_new.shape) \n",
    "weight = np.random.beta(0.2, 0.2, training_batch_size*2)\n",
    "X_weight = weight.reshape(training_batch_size*2,1,1,1)\n",
    "for i in tqdm(range(training_batch_size*2),desc='mixuping'):   #对数据做mixup处理\n",
    "    index = np.random.randint(1,training_batch_size*2)\n",
    "    X_train_new=np.append(X_train_new,np.expand_dims(X_weight[i]*X_train_new[i]+(1-weight[i])*X_train_new[index],axis=0),axis=0)\n",
    "    train_label = np.append(train_label, np.expand_dims(weight[i]*train_label[i]+(1-weight[i])*train_label[index],axis=0),axis=0)\n",
    "print(X_train_new.shape) \n",
    "\n",
    "del X_train\n",
    "X_train_new = X_train_new.reshape(X_train_new.shape[0], 32, 32, 32, 1) \n",
    "#print(X_train_new.shape)\n",
    "train_label = keras.utils.to_categorical(train_label, 2)\n",
    "print(train_label.shape)\n",
    "\n",
    "for root, dirs,files in os.walk('test'):   \n",
    "    filename = files\n",
    "\n",
    "def sort_key(s):\n",
    "    return int(s[9:-4])\n",
    "\n",
    "test_number = sorted(filename,key = sort_key)         #获得排序的测试集\n",
    "\n",
    "#读取测试数据\n",
    "voxel_test = []     #用于存储测试数据的voxel\n",
    "seg_test = []       #用于存储测试数据的seg\n",
    "\n",
    "for i in tqdm(range(584), desc='reading test_data'):    #写入测试数据的进度\n",
    "    try:\n",
    "        tmp = np.load('test/candidate{}.npz'.format(i)) #依次读取测试数据中的candidate{i}文件\n",
    "    except FileNotFoundError:                           #无该文件时直接进入下一次循环\n",
    "        continue\n",
    "    try:\n",
    "        voxel_test = np.append(voxel_test, np.expand_dims(tmp['voxel'], axis=0), axis=0)    #向voxel_test中添加读取的voxel向量\n",
    "        seg_test = np.append(seg_test, np.expand_dims(tmp['seg'], axis=0), axis=0)          #向seg_test中添加读取的seg向量\n",
    "    except ValueError:\n",
    "        voxel_test = np.expand_dims(tmp['voxel'], axis=0)   #写入初次读取的voxel\n",
    "        seg_test = np.expand_dims(tmp['seg'], axis=0)       #写入初次读取的seg\n",
    "\n",
    "seg_test = seg_test.astype(np.int)      #将seg布尔array转换为1/0整数\n",
    "X_test= voxel_test*seg_test             #抠取结节\n",
    "\n",
    "X_test=X_test.astype(np.float32)\n",
    "X_test/=128.-1.\n",
    "\n",
    "test_batch_size = X_test.shape[0]  #测试数据集的数量\n",
    "#print(X_test.shape)\n",
    "\n",
    "X_test_new=crop(X_test[0],(50,50,50),(32,32,32))\n",
    "\n",
    "X_test_new=np.expand_dims(X_test_new,axis=0)\n",
    "#print(X_test_new.shape) \n",
    "\n",
    "for i in tqdm(range(test_batch_size-1),desc='croping'):\n",
    "    X_test_new=np.append(X_test_new,np.expand_dims(crop(X_test[i+1],(50,50,50),(32,32,32)),axis=0),axis=0)\n",
    "#print(X_test_new.shape)   \n",
    "del X_test\n",
    "X_test_new = X_test_new.reshape(X_test_new.shape[0], 32, 32, 32, 1)     #将测试数据集整合成5d张量\n",
    "print(X_test_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters: {'activation': <function <lambda> at 0x7ff488a4c730>, 'bn_scale': True, 'weight_decay': 0.0, 'kernel_initializer': 'he_uniform', 'first_scale': None, 'dhw': [32, 32, 32], 'k': 16, 'bottleneck': 2, 'compression': 2, 'first_layer': 32, 'down_structure': [4, 4], 'output_size': 2}\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/rapids/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 32, 1 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 32, 32, 32, 3 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 32, 3 128         conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 32, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 32, 32, 32, 3 1024        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 32, 3 128         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 32, 3 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 32, 32, 32, 1 13840       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 32, 4 0           conv3d_3[0][0]                   \n",
      "                                                                 conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32, 4 192         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 32, 4 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 32, 32, 32, 3 1536        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32, 3 128         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32, 3 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 32, 32, 32, 1 13840       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 32, 6 0           conv3d_5[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32, 6 256         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32, 6 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 32, 32, 32, 3 2048        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 32, 3 128         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 32, 3 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 32, 32, 32, 1 13840       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 32, 8 0           conv3d_7[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 32, 8 320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 32, 8 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 32, 32, 32, 3 2560        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 32, 3 128         conv3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 32, 3 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 32, 32, 32, 1 13840       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 32, 9 0           conv3d_9[0][0]                   \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 32, 9 384         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 32, 9 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 32, 32, 32, 4 4656        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling3d_1 (AveragePoo (None, 16, 16, 16, 4 0           conv3d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 16, 4 192         average_pooling3d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 16, 4 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)              (None, 16, 16, 16, 3 1536        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 16, 3 128         conv3d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 16, 3 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 16, 16, 16, 1 13840       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 16, 16, 6 0           conv3d_12[0][0]                  \n",
      "                                                                 average_pooling3d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 16, 6 256         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 16, 6 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 16, 16, 16, 3 2048        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 16, 3 128         conv3d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 16, 3 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)              (None, 16, 16, 16, 1 13840       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 16, 16, 16, 8 0           conv3d_14[0][0]                  \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 16, 8 320         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 16, 8 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)              (None, 16, 16, 16, 3 2560        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 16, 3 128         conv3d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 16, 3 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 16, 16, 16, 1 13840       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 16, 9 0           conv3d_16[0][0]                  \n",
      "                                                                 concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 16, 9 384         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 16, 9 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_17 (Conv3D)              (None, 16, 16, 16, 3 3072        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 16, 3 128         conv3d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 16, 3 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_18 (Conv3D)              (None, 16, 16, 16, 1 13840       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 16, 1 0           conv3d_18[0][0]                  \n",
      "                                                                 concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 16, 1 448         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 16, 1 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling3d_1 (Glo (None, 112)          0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            226         global_average_pooling3d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 136,786\n",
      "Trainable params: 134,834\n",
      "Non-trainable params: 1,952\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/rapids/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1700 samples, validate on 160 samples\n",
      "Epoch 1/26\n",
      "1700/1700 [==============================] - 64s 38ms/step - loss: 0.6646 - acc: 0.6171 - val_loss: 0.6996 - val_acc: 0.6875\n",
      "Epoch 2/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6521 - acc: 0.6141 - val_loss: 0.6275 - val_acc: 0.6813\n",
      "Epoch 3/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6392 - acc: 0.6306 - val_loss: 0.8515 - val_acc: 0.5813\n",
      "Epoch 4/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6428 - acc: 0.6188 - val_loss: 0.6487 - val_acc: 0.6750\n",
      "Epoch 5/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6335 - acc: 0.6341 - val_loss: 0.6444 - val_acc: 0.6688\n",
      "Epoch 6/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6286 - acc: 0.6435 - val_loss: 0.7313 - val_acc: 0.4938\n",
      "Epoch 7/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6281 - acc: 0.6353 - val_loss: 1.7523 - val_acc: 0.4438\n",
      "Epoch 8/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6243 - acc: 0.6441 - val_loss: 0.6340 - val_acc: 0.6875\n",
      "Epoch 9/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6278 - acc: 0.6365 - val_loss: 0.8594 - val_acc: 0.5688\n",
      "Epoch 10/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6236 - acc: 0.6535 - val_loss: 0.9269 - val_acc: 0.4188\n",
      "Epoch 11/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6157 - acc: 0.6406 - val_loss: 0.7887 - val_acc: 0.6625\n",
      "Epoch 12/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6174 - acc: 0.6441 - val_loss: 0.8000 - val_acc: 0.6625\n",
      "Epoch 13/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6201 - acc: 0.6465 - val_loss: 0.6218 - val_acc: 0.6687\n",
      "Epoch 14/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6055 - acc: 0.6629 - val_loss: 0.5989 - val_acc: 0.6687\n",
      "Epoch 15/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6026 - acc: 0.6671 - val_loss: 0.5948 - val_acc: 0.6875\n",
      "Epoch 16/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.5971 - acc: 0.6741 - val_loss: 0.6397 - val_acc: 0.6000\n",
      "Epoch 17/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.6008 - acc: 0.6712 - val_loss: 0.6183 - val_acc: 0.6750\n",
      "Epoch 18/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.5976 - acc: 0.6647 - val_loss: 0.6429 - val_acc: 0.7125\n",
      "Epoch 19/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.5874 - acc: 0.6818 - val_loss: 0.8186 - val_acc: 0.4563\n",
      "Epoch 20/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.5855 - acc: 0.6812 - val_loss: 0.6146 - val_acc: 0.6750\n",
      "Epoch 21/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.5829 - acc: 0.6871 - val_loss: 0.6074 - val_acc: 0.6875\n",
      "Epoch 22/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.5799 - acc: 0.7000 - val_loss: 0.5977 - val_acc: 0.7250\n",
      "Epoch 23/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.5837 - acc: 0.6906 - val_loss: 0.6522 - val_acc: 0.6813\n",
      "Epoch 24/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.5704 - acc: 0.6976 - val_loss: 0.6805 - val_acc: 0.6250\n",
      "Epoch 25/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.5725 - acc: 0.7041 - val_loss: 0.5730 - val_acc: 0.7312\n",
      "Epoch 26/26\n",
      "1700/1700 [==============================] - 57s 33ms/step - loss: 0.5701 - acc: 0.7065 - val_loss: 0.5704 - val_acc: 0.7250\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "   # with tf.device('/cpu:3'):\n",
    "    X_train_new,x_val, train_label, y_val =train_test_split(X_train_new,train_label,test_size=160,shuffle=True)\n",
    "    model = get_model(weights=None)\n",
    "    #parallel_model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adamax',\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    filepath=\"model_weight.h5\"\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='acc', factor=0.2,patience=3, min_lr=0.0005)\n",
    "    checkpoint = ModelCheckpoint(filepath,monitor='val_loss',verbose=0,save_best_only=True,mode='min',period=1)\n",
    "    callbacks_list=[reduce_lr,\n",
    "                    checkpoint,\n",
    "                   ]\n",
    "    model.fit(X_train_new,train_label,batch_size=30,epochs=26,validation_data=(x_val,y_val),callbacks=callbacks_list,shuffle=True)\n",
    "    #保存模型\n",
    "    model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测并存储结果\n",
    "import numpy  \n",
    "y_pred=model.predict(X_test_new)\n",
    "\n",
    "test_label = []\n",
    "test_label.append(['id','Predicted'])\n",
    "for i in range(test_batch_size):\n",
    "    test_label.append([test_number[i][:-4],y_pred[i][1]])\n",
    "    \n",
    "with open('Submission.csv', 'w',newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
